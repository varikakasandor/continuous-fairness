{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>ERM with DNN under penalty of Equalized Odds</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement here a regular Empirical Risk Minimization (ERM) of a Deep Neural Network (DNN) penalized to enforce an Equalized Odds constraint. More formally, given a dataset of size $n$ consisting of context features $x$, target $y$ and a sensitive information $a$ to protect, we want to solve\n",
    "$$\n",
    "\\text{argmin}_{h\\in\\mathcal{H}}\\frac{1}{n}\\sum_{i=1}^n \\ell(y_i, h(x_i)) + \\lambda \\chi^2|_1\n",
    "$$\n",
    "where $\\ell$ is for instance the MSE and the penalty is\n",
    "$$\n",
    "\\chi^2|_1 = \\left\\lVert\\chi^2\\left(\\hat{\\pi}(h(x)|y, a|y), \\hat{\\pi}(h(x)|y)\\otimes\\hat{\\pi}(a|y)\\right)\\right\\rVert_1\n",
    "$$\n",
    "where $\\hat{\\pi}$ denotes the empirical density estimated through a Gaussian KDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('../..')))\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "\n",
    "from examples.data_loading import read_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "\n",
    "We use here the _communities and crimes_ dataset that can be found on the UCI Machine Learning Repository (http://archive.ics.uci.edu/ml/datasets/communities+and+crime). Non-predictive information, such as city name, state... have been removed and the file is at the arff format for ease of loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, a_train, x_test, y_test, a_test = read_dataset(name='crimes', fold=1)\n",
    "n, d = x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Deep Neural Network\n",
    "\n",
    "We define a very simple DNN for regression here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NetRegression, self).__init__()\n",
    "        size = 50\n",
    "        self.first = nn.Linear(input_size, size)\n",
    "        self.fc = nn.Linear(size, size)\n",
    "        self.last = nn.Linear(size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.selu(self.first(x))\n",
    "        out = F.selu(self.fc(out))\n",
    "        out = self.last(out)\n",
    "        out = torch.sigmoid(out) # NEW\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fairness-penalized ERM\n",
    "\n",
    "We now implement the full learning loop. The regression loss used is the quadratic loss with a L2 regularization and the fairness-inducing penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_learning(x_train, y_train, a_train, model, fairness_metric_train, fairness_metric_test, fairness_weight = 1.0, lr=1e-5, num_epochs=10, print_progress = True):    \n",
    "    X = torch.tensor(x_train.astype(np.float32))\n",
    "    A = torch.tensor(a_train.astype(np.float32))\n",
    "    Y = torch.tensor(y_train.astype(np.float32))\n",
    "    dataset = data_utils.TensorDataset(X, Y, A)\n",
    "    dataset_loader = data_utils.DataLoader(dataset=dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "    # mse regression objective\n",
    "    data_fitting_loss = nn.MSELoss()\n",
    "\n",
    "    # stochastic optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "\n",
    "    for j in range(num_epochs):\n",
    "        if print_progress:\n",
    "            print(f\"EPOCH {j} started\")\n",
    "        for i, (x, y, a) in enumerate(dataset_loader):\n",
    "            # if print_progress:\n",
    "            #    print(f\"Batch {i} started\")\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                prediction = model(x).flatten()\n",
    "                loss = fairness_weight * fairness_metric_train(prediction, a, y) + data_fitting_loss(prediction, y)\n",
    "                loss.backward()\n",
    "                \"\"\"for name, param in model.named_parameters():\n",
    "                    if param.grad is not None:\n",
    "                        print(f\"Parameter: {name}\\nGradient: {param.grad}\\n\")\"\"\"\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "        mse_curr_test, nd_curr_test = evaluate(model, x_test, y_test, a_test, fairness_metric=fairness_metric_test)\n",
    "        print(f\"TEST -- mse: {mse_curr_test}, nd: {nd_curr_test}, combined: {mse_curr_test + fairness_weight * nd_curr_test}\")\n",
    "        mse_curr_train, nd_curr_train = evaluate(model, x_train, y_train, a_train, fairness_metric=fairness_metric_test)\n",
    "        print(f\"TRAIN -- mse: {mse_curr_train}, nd: {nd_curr_train}, combined: {mse_curr_train + fairness_weight * nd_curr_train}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "For the evaluation on the test set, we compute two metrics: the MSE (accuracy) and HGR$|_\\infty$ (fairness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x, y, a, fairness_metric):\n",
    "    X = torch.tensor(x.astype(np.float32))\n",
    "    A = torch.Tensor(a.astype(np.float32))\n",
    "    Y = torch.tensor(y.astype(np.float32))\n",
    "\n",
    "    prediction = model(X).detach().flatten()\n",
    "    loss = nn.MSELoss()(prediction, Y)\n",
    "    discrimination = fairness_metric(prediction, A, Y)\n",
    "    return loss.item(), discrimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running everything together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fairness_metric(constrained_intervals_A, quantizition_intervals_Y, train, size_compensation = lambda x: np.sqrt(x)):\n",
    "    \n",
    "    def inside(num, endpoints):\n",
    "        start, end = endpoints\n",
    "        return start <= num and num < end\n",
    "    \n",
    "    def fairness_metric(Y_hat, A, Y):\n",
    "        nd_losses = []\n",
    "        n = len(Y_hat)\n",
    "        for inter_Y in quantizition_intervals_Y:\n",
    "            for inter_A in constrained_intervals_A:\n",
    "                cnt_y_a = 0\n",
    "                cnt_y = 0\n",
    "                sum_y_yhat = 0\n",
    "                sum_y_a_yhat = 0\n",
    "                for i in range(len(Y_hat)): # could be sped up by combining with outer loop\n",
    "                    if inside(Y[i], inter_Y):\n",
    "                        cnt_y += 1\n",
    "                        sum_y_yhat += Y_hat[i]\n",
    "                        if inside(A[i], inter_A):\n",
    "                            cnt_y_a += 1\n",
    "                            sum_y_a_yhat += Y_hat[i]\n",
    "                if cnt_y_a > 0 and cnt_y > 0:\n",
    "                    curr_nd_loss = torch.abs(sum_y_a_yhat / cnt_y_a - sum_y_yhat / cnt_y) * size_compensation(cnt_y_a / n)\n",
    "                    nd_losses.append(curr_nd_loss)\n",
    "        nd_losses_torch = torch.stack(nd_losses)\n",
    "        return torch.mean(nd_losses_torch) if train else torch.max(nd_losses_torch)\n",
    "    return fairness_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_constrained_intervals(num_constrained_intervals):\n",
    "    endpoints = np.linspace(0, 1, num_constrained_intervals + 1)\n",
    "    constrained_intervals = []\n",
    "    for i in range(len(endpoints) - 1):\n",
    "        constrained_intervals.append((endpoints[i], endpoints[i + 1]))\n",
    "    return constrained_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 started\n",
      "TEST -- mse: 0.13169682025909424, nd: 0.018028298392891884, combined: 0.14972512423992157\n",
      "TRAIN -- mse: 0.13208803534507751, nd: 0.014867031946778297, combined: 0.14695507287979126\n",
      "EPOCH 1 started\n",
      "TEST -- mse: 0.1298317015171051, nd: 0.01720607280731201, combined: 0.14703777432441711\n",
      "TRAIN -- mse: 0.13022999465465546, nd: 0.014116917736828327, combined: 0.1443469077348709\n",
      "EPOCH 2 started\n",
      "TEST -- mse: 0.12800323963165283, nd: 0.01639064960181713, combined: 0.1443938910961151\n",
      "TRAIN -- mse: 0.12840695679187775, nd: 0.013366438448429108, combined: 0.14177340269088745\n",
      "EPOCH 3 started\n",
      "TEST -- mse: 0.1262025088071823, nd: 0.015578131191432476, combined: 0.14178064465522766\n",
      "TRAIN -- mse: 0.12660987675189972, nd: 0.012618571519851685, combined: 0.1392284482717514\n",
      "EPOCH 4 started\n",
      "TEST -- mse: 0.12446454912424088, nd: 0.014787674881517887, combined: 0.13925223052501678\n",
      "TRAIN -- mse: 0.12487106025218964, nd: 0.01188971009105444, combined: 0.13676077127456665\n",
      "EPOCH 5 started\n",
      "TEST -- mse: 0.12276812642812729, nd: 0.01400845404714346, combined: 0.13677658140659332\n",
      "TRAIN -- mse: 0.1231771782040596, nd: 0.011172504164278507, combined: 0.13434968888759613\n",
      "EPOCH 6 started\n",
      "TEST -- mse: 0.12111159414052963, nd: 0.013196844607591629, combined: 0.13430844247341156\n",
      "TRAIN -- mse: 0.12152938544750214, nd: 0.010434409603476524, combined: 0.1319637894630432\n",
      "EPOCH 7 started\n",
      "TEST -- mse: 0.11947853118181229, nd: 0.012426304630935192, combined: 0.13190484046936035\n",
      "TRAIN -- mse: 0.11989563703536987, nd: 0.009721892885863781, combined: 0.12961752712726593\n",
      "EPOCH 8 started\n",
      "TEST -- mse: 0.11789710819721222, nd: 0.01163553074002266, combined: 0.12953263521194458\n",
      "TRAIN -- mse: 0.11831430345773697, nd: 0.008999651297926903, combined: 0.12731395661830902\n",
      "EPOCH 9 started\n",
      "TEST -- mse: 0.11635572463274002, nd: 0.011353579349815845, combined: 0.127709299325943\n",
      "TRAIN -- mse: 0.11677763611078262, nd: 0.008295271545648575, combined: 0.1250729113817215\n",
      "EPOCH 10 started\n",
      "TEST -- mse: 0.11483631283044815, nd: 0.01108886394649744, combined: 0.1259251832962036\n",
      "TRAIN -- mse: 0.11526499688625336, nd: 0.0075867376290261745, combined: 0.12285173684358597\n",
      "EPOCH 11 started\n",
      "TEST -- mse: 0.11337296664714813, nd: 0.010828295722603798, combined: 0.12420126050710678\n",
      "TRAIN -- mse: 0.1138065978884697, nd: 0.006900778505951166, combined: 0.12070737779140472\n",
      "EPOCH 12 started\n",
      "TEST -- mse: 0.111943818628788, nd: 0.010562699288129807, combined: 0.1225065141916275\n",
      "TRAIN -- mse: 0.11238785088062286, nd: 0.006223352625966072, combined: 0.11861120164394379\n",
      "EPOCH 13 started\n",
      "TEST -- mse: 0.11056164652109146, nd: 0.010293150320649147, combined: 0.12085479497909546\n",
      "TRAIN -- mse: 0.11101506650447845, nd: 0.005560836289077997, combined: 0.11657590419054031\n",
      "EPOCH 14 started\n",
      "TEST -- mse: 0.10919816792011261, nd: 0.010029235854744911, combined: 0.11922740191221237\n",
      "TRAIN -- mse: 0.1096586063504219, nd: 0.004904883448034525, combined: 0.11456348747015\n",
      "EPOCH 15 started\n",
      "TEST -- mse: 0.1078890711069107, nd: 0.009763446636497974, combined: 0.1176525205373764\n",
      "TRAIN -- mse: 0.10835909843444824, nd: 0.004255891311913729, combined: 0.11261498928070068\n",
      "EPOCH 16 started\n",
      "TEST -- mse: 0.1066046729683876, nd: 0.009499089792370796, combined: 0.11610376089811325\n",
      "TRAIN -- mse: 0.10708598792552948, nd: 0.003612336702644825, combined: 0.11069832742214203\n",
      "EPOCH 17 started\n",
      "TEST -- mse: 0.1054038554430008, nd: 0.00927036628127098, combined: 0.11467422544956207\n",
      "TRAIN -- mse: 0.1058940514922142, nd: 0.0030404950957745314, combined: 0.10893454402685165\n",
      "EPOCH 18 started\n",
      "TEST -- mse: 0.10423143208026886, nd: 0.009016442112624645, combined: 0.11324787139892578\n",
      "TRAIN -- mse: 0.10473396629095078, nd: 0.002444707090035081, combined: 0.10717867314815521\n",
      "EPOCH 19 started\n",
      "TEST -- mse: 0.10307039320468903, nd: 0.008760976605117321, combined: 0.11183136701583862\n",
      "TRAIN -- mse: 0.10358832776546478, nd: 0.0018569055246189237, combined: 0.10544523596763611\n",
      "EPOCH 20 started\n",
      "TEST -- mse: 0.10194845497608185, nd: 0.008498476818203926, combined: 0.11044692993164062\n",
      "TRAIN -- mse: 0.10248515009880066, nd: 0.0012802369892597198, combined: 0.10376538336277008\n",
      "EPOCH 21 started\n",
      "TEST -- mse: 0.10083978623151779, nd: 0.008242601528763771, combined: 0.10908238589763641\n",
      "TRAIN -- mse: 0.10140284895896912, nd: 0.0009823598666116595, combined: 0.10238520801067352\n",
      "EPOCH 22 started\n",
      "TEST -- mse: 0.09985736757516861, nd: 0.008037726394832134, combined: 0.10789509117603302\n",
      "TRAIN -- mse: 0.10043120384216309, nd: 0.0008767672115936875, combined: 0.10130797326564789\n",
      "EPOCH 23 started\n",
      "TEST -- mse: 0.09888502955436707, nd: 0.007826777175068855, combined: 0.10671180486679077\n",
      "TRAIN -- mse: 0.09947071969509125, nd: 0.0007739830180071294, combined: 0.10024470090866089\n",
      "EPOCH 24 started\n",
      "TEST -- mse: 0.0979766845703125, nd: 0.007633544970303774, combined: 0.10561022907495499\n",
      "TRAIN -- mse: 0.09856189042329788, nd: 0.0006905226618982852, combined: 0.09925241023302078\n",
      "EPOCH 25 started\n",
      "TEST -- mse: 0.0971231460571289, nd: 0.007452221121639013, combined: 0.10457536578178406\n",
      "TRAIN -- mse: 0.09770334511995316, nd: 0.001036267844028771, combined: 0.0987396165728569\n",
      "EPOCH 26 started\n",
      "TEST -- mse: 0.09630343317985535, nd: 0.007259936537593603, combined: 0.10356336832046509\n",
      "TRAIN -- mse: 0.0968797504901886, nd: 0.0013982630334794521, combined: 0.09827801585197449\n",
      "EPOCH 27 started\n",
      "TEST -- mse: 0.09549295157194138, nd: 0.007083673495799303, combined: 0.10257662832736969\n",
      "TRAIN -- mse: 0.09606609493494034, nd: 0.0017613657983019948, combined: 0.09782746434211731\n",
      "EPOCH 28 started\n",
      "TEST -- mse: 0.09468967467546463, nd: 0.006904846988618374, combined: 0.10159452259540558\n",
      "TRAIN -- mse: 0.09525997191667557, nd: 0.0021316618658602238, combined: 0.09739163517951965\n",
      "EPOCH 29 started\n",
      "TEST -- mse: 0.0939505472779274, nd: 0.0067575122229754925, combined: 0.10070805996656418\n",
      "TRAIN -- mse: 0.09451817721128464, nd: 0.0024511830415576696, combined: 0.0969693586230278\n",
      "EPOCH 30 started\n",
      "TEST -- mse: 0.09321320801973343, nd: 0.006599032785743475, combined: 0.09981223940849304\n",
      "TRAIN -- mse: 0.09378261119127274, nd: 0.0027840021066367626, combined: 0.09656661003828049\n",
      "EPOCH 31 started\n",
      "TEST -- mse: 0.09247305989265442, nd: 0.006426407489925623, combined: 0.0988994687795639\n",
      "TRAIN -- mse: 0.09304516017436981, nd: 0.0031511452980339527, combined: 0.09619630873203278\n",
      "EPOCH 32 started\n",
      "TEST -- mse: 0.09175873547792435, nd: 0.006243702489882708, combined: 0.09800244122743607\n",
      "TRAIN -- mse: 0.0923323705792427, nd: 0.003490981413051486, combined: 0.09582335501909256\n",
      "EPOCH 33 started\n",
      "TEST -- mse: 0.09111057966947556, nd: 0.006109632086008787, combined: 0.09722021222114563\n",
      "TRAIN -- mse: 0.09167886525392532, nd: 0.0037541391793638468, combined: 0.09543300420045853\n",
      "EPOCH 34 started\n",
      "TEST -- mse: 0.09047103673219681, nd: 0.005978393834084272, combined: 0.09644942730665207\n",
      "TRAIN -- mse: 0.09103551506996155, nd: 0.0040077874436974525, combined: 0.09504330158233643\n",
      "EPOCH 35 started\n",
      "TEST -- mse: 0.08984170109033585, nd: 0.005892151966691017, combined: 0.09573385119438171\n",
      "TRAIN -- mse: 0.09040453284978867, nd: 0.004275589715689421, combined: 0.09468012303113937\n",
      "EPOCH 36 started\n",
      "TEST -- mse: 0.08926673233509064, nd: 0.005813713185489178, combined: 0.09508044272661209\n",
      "TRAIN -- mse: 0.08982079476118088, nd: 0.004502082243561745, combined: 0.09432287514209747\n",
      "EPOCH 37 started\n",
      "TEST -- mse: 0.08867662400007248, nd: 0.0057249171659350395, combined: 0.0944015383720398\n",
      "TRAIN -- mse: 0.08922919631004333, nd: 0.004756479524075985, combined: 0.0939856767654419\n",
      "EPOCH 38 started\n",
      "TEST -- mse: 0.08810780197381973, nd: 0.005644329823553562, combined: 0.09375213086605072\n",
      "TRAIN -- mse: 0.0886543020606041, nd: 0.004995086695998907, combined: 0.09364938735961914\n",
      "EPOCH 39 started\n",
      "TEST -- mse: 0.08757764846086502, nd: 0.005588528700172901, combined: 0.09316617995500565\n",
      "TRAIN -- mse: 0.08811132609844208, nd: 0.005175234284251928, combined: 0.09328655898571014\n",
      "EPOCH 40 started\n",
      "TEST -- mse: 0.08702928572893143, nd: 0.005513020791113377, combined: 0.09254230558872223\n",
      "TRAIN -- mse: 0.0875583365559578, nd: 0.0053932624869048595, combined: 0.09295159578323364\n",
      "EPOCH 41 started\n",
      "TEST -- mse: 0.0864996686577797, nd: 0.005432499572634697, combined: 0.09193217009305954\n",
      "TRAIN -- mse: 0.08702327311038971, nd: 0.005621957592666149, combined: 0.09264522790908813\n",
      "EPOCH 42 started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- mse: 0.08600173145532608, nd: 0.005367509555071592, combined: 0.09136924147605896\n",
      "TRAIN -- mse: 0.08651749044656754, nd: 0.005812514573335648, combined: 0.09233000874519348\n",
      "EPOCH 43 started\n",
      "TEST -- mse: 0.08550184220075607, nd: 0.005306053441017866, combined: 0.09080789238214493\n",
      "TRAIN -- mse: 0.08600928634405136, nd: 0.0059921215288341045, combined: 0.09200140833854675\n",
      "EPOCH 44 started\n",
      "TEST -- mse: 0.0849839597940445, nd: 0.0052267382852733135, combined: 0.0902106985449791\n",
      "TRAIN -- mse: 0.08548987656831741, nd: 0.006220520474016666, combined: 0.0917103961110115\n",
      "EPOCH 45 started\n",
      "TEST -- mse: 0.08449902385473251, nd: 0.005157111212611198, combined: 0.08965613692998886\n",
      "TRAIN -- mse: 0.08500072360038757, nd: 0.006417182274162769, combined: 0.09141790866851807\n",
      "EPOCH 46 started\n",
      "TEST -- mse: 0.08400703221559525, nd: 0.0053914510644972324, combined: 0.08939848095178604\n",
      "TRAIN -- mse: 0.08450458943843842, nd: 0.006640880834311247, combined: 0.09114547073841095\n",
      "EPOCH 47 started\n",
      "TEST -- mse: 0.08353012055158615, nd: 0.005608432926237583, combined: 0.08913855254650116\n",
      "TRAIN -- mse: 0.08402268588542938, nd: 0.006851173937320709, combined: 0.09087385982275009\n",
      "EPOCH 48 started\n",
      "TEST -- mse: 0.0830458253622055, nd: 0.005811520852148533, combined: 0.08885734528303146\n",
      "TRAIN -- mse: 0.08353608846664429, nd: 0.007046426180750132, combined: 0.09058251231908798\n",
      "EPOCH 49 started\n",
      "TEST -- mse: 0.08256339281797409, nd: 0.0060419063083827496, combined: 0.08860529959201813\n",
      "TRAIN -- mse: 0.08305412530899048, nd: 0.007255517411977053, combined: 0.09030964225530624\n",
      "EPOCH 50 started\n",
      "TEST -- mse: 0.0821244940161705, nd: 0.0061995480209589005, combined: 0.08832404017448425\n",
      "TRAIN -- mse: 0.08260593563318253, nd: 0.00741494121029973, combined: 0.09002088010311127\n",
      "EPOCH 51 started\n",
      "TEST -- mse: 0.08168987184762955, nd: 0.0063657257705926895, combined: 0.08805559575557709\n",
      "TRAIN -- mse: 0.08216622471809387, nd: 0.007577247451990843, combined: 0.089743472635746\n",
      "EPOCH 52 started\n",
      "TEST -- mse: 0.08125206083059311, nd: 0.006561448331922293, combined: 0.08781351149082184\n",
      "TRAIN -- mse: 0.08172179013490677, nd: 0.00776548869907856, combined: 0.08948727697134018\n",
      "EPOCH 53 started\n",
      "TEST -- mse: 0.08083434402942657, nd: 0.006735231261700392, combined: 0.08756957203149796\n",
      "TRAIN -- mse: 0.0812944620847702, nd: 0.007936844602227211, combined: 0.08923130482435226\n",
      "EPOCH 54 started\n",
      "TEST -- mse: 0.08041435480117798, nd: 0.006912916898727417, combined: 0.0873272716999054\n",
      "TRAIN -- mse: 0.08087073266506195, nd: 0.008105362765491009, combined: 0.08897609263658524\n",
      "EPOCH 55 started\n",
      "TEST -- mse: 0.07999875396490097, nd: 0.007097321096807718, combined: 0.08709607273340225\n",
      "TRAIN -- mse: 0.08044790476560593, nd: 0.008279462344944477, combined: 0.08872736990451813\n",
      "EPOCH 56 started\n",
      "TEST -- mse: 0.07958429306745529, nd: 0.007279472891241312, combined: 0.08686376363039017\n",
      "TRAIN -- mse: 0.08002520352602005, nd: 0.008452191017568111, combined: 0.08847739547491074\n",
      "EPOCH 57 started\n",
      "TEST -- mse: 0.07918134331703186, nd: 0.0074560437351465225, combined: 0.08663738518953323\n",
      "TRAIN -- mse: 0.07961969822645187, nd: 0.008615554310381413, combined: 0.08823525160551071\n",
      "EPOCH 58 started\n",
      "TEST -- mse: 0.07879652827978134, nd: 0.007620604708790779, combined: 0.08641713112592697\n",
      "TRAIN -- mse: 0.07922764867544174, nd: 0.008771107532083988, combined: 0.08799875527620316\n",
      "EPOCH 59 started\n",
      "TEST -- mse: 0.07844281196594238, nd: 0.007721091620624065, combined: 0.08616390079259872\n",
      "TRAIN -- mse: 0.07886529713869095, nd: 0.008873023092746735, combined: 0.08773832023143768\n",
      "EPOCH 60 started\n",
      "TEST -- mse: 0.07806089520454407, nd: 0.007851123809814453, combined: 0.08591201901435852\n",
      "TRAIN -- mse: 0.07847672700881958, nd: 0.009006492793560028, combined: 0.08748321980237961\n",
      "EPOCH 61 started\n",
      "TEST -- mse: 0.07769417017698288, nd: 0.008001874200999737, combined: 0.08569604158401489\n",
      "TRAIN -- mse: 0.07810240983963013, nd: 0.009154997766017914, combined: 0.08725740760564804\n",
      "EPOCH 62 started\n",
      "TEST -- mse: 0.0773390680551529, nd: 0.008132575079798698, combined: 0.08547164499759674\n",
      "TRAIN -- mse: 0.07774478197097778, nd: 0.009279335848987103, combined: 0.08702411502599716\n",
      "EPOCH 63 started\n",
      "TEST -- mse: 0.07697729766368866, nd: 0.008288315497338772, combined: 0.08526561409235\n",
      "TRAIN -- mse: 0.07737951725721359, nd: 0.009424152784049511, combined: 0.08680366724729538\n",
      "EPOCH 64 started\n",
      "TEST -- mse: 0.07662605494260788, nd: 0.008421982638537884, combined: 0.08504803478717804\n",
      "TRAIN -- mse: 0.07702240347862244, nd: 0.00955345667898655, combined: 0.08657585829496384\n",
      "EPOCH 65 started\n",
      "TEST -- mse: 0.07628197968006134, nd: 0.008578949607908726, combined: 0.08486092835664749\n",
      "TRAIN -- mse: 0.07667580991983414, nd: 0.009688845835626125, combined: 0.08636465668678284\n",
      "EPOCH 66 started\n",
      "TEST -- mse: 0.07591433823108673, nd: 0.008804752491414547, combined: 0.08471909165382385\n",
      "TRAIN -- mse: 0.07631038129329681, nd: 0.00987989455461502, combined: 0.08619027584791183\n",
      "EPOCH 67 started\n",
      "TEST -- mse: 0.07559020072221756, nd: 0.008924045599997044, combined: 0.08451424539089203\n",
      "TRAIN -- mse: 0.07597941160202026, nd: 0.009994370862841606, combined: 0.08597378432750702\n",
      "EPOCH 68 started\n",
      "TEST -- mse: 0.07526877522468567, nd: 0.009038332849740982, combined: 0.08430710434913635\n",
      "TRAIN -- mse: 0.07565245032310486, nd: 0.010101678781211376, combined: 0.08575412631034851\n",
      "EPOCH 69 started\n",
      "TEST -- mse: 0.07495817542076111, nd: 0.009134599007666111, combined: 0.08409277349710464\n",
      "TRAIN -- mse: 0.07533080130815506, nd: 0.010204330086708069, combined: 0.08553513139486313\n",
      "EPOCH 70 started\n",
      "TEST -- mse: 0.074663445353508, nd: 0.009222619235515594, combined: 0.08388606458902359\n",
      "TRAIN -- mse: 0.07502231746912003, nd: 0.010295898653566837, combined: 0.08531821519136429\n",
      "EPOCH 71 started\n",
      "TEST -- mse: 0.0743555948138237, nd: 0.009345447644591331, combined: 0.08370104432106018\n",
      "TRAIN -- mse: 0.0747067779302597, nd: 0.010413276962935925, combined: 0.0851200520992279\n",
      "EPOCH 72 started\n",
      "TEST -- mse: 0.07404650002717972, nd: 0.009459238499403, combined: 0.08350573480129242\n",
      "TRAIN -- mse: 0.07439316064119339, nd: 0.010524103417992592, combined: 0.08491726219654083\n",
      "EPOCH 73 started\n",
      "TEST -- mse: 0.07372992485761642, nd: 0.009581102058291435, combined: 0.08331102877855301\n",
      "TRAIN -- mse: 0.07406716793775558, nd: 0.010643973015248775, combined: 0.08471114188432693\n",
      "EPOCH 74 started\n",
      "TEST -- mse: 0.07342942804098129, nd: 0.009694786742329597, combined: 0.08312421292066574\n",
      "TRAIN -- mse: 0.07376141101121902, nd: 0.010748493485152721, combined: 0.08450990170240402\n",
      "EPOCH 75 started\n",
      "TEST -- mse: 0.07314291596412659, nd: 0.009779896587133408, combined: 0.08292281627655029\n",
      "TRAIN -- mse: 0.07347291707992554, nd: 0.01082789059728384, combined: 0.08430080860853195\n",
      "EPOCH 76 started\n",
      "TEST -- mse: 0.07286009192466736, nd: 0.009869958274066448, combined: 0.08273004740476608\n",
      "TRAIN -- mse: 0.07318481057882309, nd: 0.010911517776548862, combined: 0.08409632742404938\n",
      "EPOCH 77 started\n",
      "TEST -- mse: 0.0725838914513588, nd: 0.00996165256947279, combined: 0.08254554122686386\n",
      "TRAIN -- mse: 0.07289905101060867, nd: 0.011001468636095524, combined: 0.08390051871538162\n",
      "EPOCH 78 started\n",
      "TEST -- mse: 0.07229313254356384, nd: 0.010074879974126816, combined: 0.08236801624298096\n",
      "TRAIN -- mse: 0.07260388135910034, nd: 0.011104113422334194, combined: 0.08370799571275711\n",
      "EPOCH 79 started\n",
      "TEST -- mse: 0.07202016562223434, nd: 0.010154900141060352, combined: 0.08217506855726242\n",
      "TRAIN -- mse: 0.07232443243265152, nd: 0.011183995753526688, combined: 0.0835084319114685\n",
      "EPOCH 80 started\n",
      "TEST -- mse: 0.07174849510192871, nd: 0.010233419947326183, combined: 0.08198191225528717\n",
      "TRAIN -- mse: 0.0720466747879982, nd: 0.011262933723628521, combined: 0.083309605717659\n",
      "EPOCH 81 started\n",
      "TEST -- mse: 0.07146536558866501, nd: 0.010351430624723434, combined: 0.08181679248809814\n",
      "TRAIN -- mse: 0.07175857573747635, nd: 0.011371462605893612, combined: 0.08313003927469254\n",
      "EPOCH 82 started\n",
      "TEST -- mse: 0.0711885467171669, nd: 0.01045769453048706, combined: 0.08164624124765396\n",
      "TRAIN -- mse: 0.07147644460201263, nd: 0.011471306905150414, combined: 0.0829477533698082\n",
      "EPOCH 83 started\n",
      "TEST -- mse: 0.07091572880744934, nd: 0.010565546341240406, combined: 0.08148127794265747\n",
      "TRAIN -- mse: 0.0711972713470459, nd: 0.011576324701309204, combined: 0.0827735960483551\n",
      "EPOCH 84 started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- mse: 0.07063963264226913, nd: 0.010678439401090145, combined: 0.08131807297468185\n",
      "TRAIN -- mse: 0.0709143728017807, nd: 0.01168497372418642, combined: 0.08259934931993484\n",
      "EPOCH 85 started\n",
      "TEST -- mse: 0.07035481184720993, nd: 0.010836594738066196, combined: 0.08119140565395355\n",
      "TRAIN -- mse: 0.07063020765781403, nd: 0.011822276748716831, combined: 0.08245248347520828\n",
      "EPOCH 86 started\n",
      "TEST -- mse: 0.07008422911167145, nd: 0.010951923206448555, combined: 0.08103615045547485\n",
      "TRAIN -- mse: 0.07035385817289352, nd: 0.011931453831493855, combined: 0.0822853147983551\n",
      "EPOCH 87 started\n",
      "TEST -- mse: 0.06984204798936844, nd: 0.01100276131182909, combined: 0.08084481209516525\n",
      "TRAIN -- mse: 0.07010448724031448, nd: 0.01198981236666441, combined: 0.08209429681301117\n",
      "EPOCH 88 started\n",
      "TEST -- mse: 0.0695854052901268, nd: 0.011089337058365345, combined: 0.08067474514245987\n",
      "TRAIN -- mse: 0.06983985006809235, nd: 0.012076295912265778, combined: 0.08191614598035812\n",
      "EPOCH 89 started\n",
      "TEST -- mse: 0.06933802366256714, nd: 0.011152435094118118, combined: 0.08049045503139496\n",
      "TRAIN -- mse: 0.06958318501710892, nd: 0.012150085531175137, combined: 0.08173327147960663\n",
      "EPOCH 90 started\n",
      "TEST -- mse: 0.0690830871462822, nd: 0.011243823915719986, combined: 0.08032691478729248\n",
      "TRAIN -- mse: 0.06932021677494049, nd: 0.012238696217536926, combined: 0.08155891299247742\n",
      "EPOCH 91 started\n",
      "TEST -- mse: 0.06882002204656601, nd: 0.011367488652467728, combined: 0.08018751442432404\n",
      "TRAIN -- mse: 0.06905421614646912, nd: 0.012344752438366413, combined: 0.08139897137880325\n",
      "EPOCH 92 started\n",
      "TEST -- mse: 0.0685647502541542, nd: 0.011461992748081684, combined: 0.08002674579620361\n",
      "TRAIN -- mse: 0.06879255920648575, nd: 0.012437844648957253, combined: 0.08123040199279785\n",
      "EPOCH 93 started\n",
      "TEST -- mse: 0.06832115352153778, nd: 0.011548050679266453, combined: 0.07986920326948166\n",
      "TRAIN -- mse: 0.06854202598333359, nd: 0.012514781206846237, combined: 0.08105680346488953\n",
      "EPOCH 94 started\n",
      "TEST -- mse: 0.06808153539896011, nd: 0.011645934544503689, combined: 0.07972747087478638\n",
      "TRAIN -- mse: 0.06829745322465897, nd: 0.01260293833911419, combined: 0.0809003934264183\n",
      "EPOCH 95 started\n",
      "TEST -- mse: 0.06785944849252701, nd: 0.011715807020664215, combined: 0.07957525551319122\n",
      "TRAIN -- mse: 0.06806790083646774, nd: 0.012666810303926468, combined: 0.08073471486568451\n",
      "EPOCH 96 started\n",
      "TEST -- mse: 0.06762047111988068, nd: 0.011783510446548462, combined: 0.07940398156642914\n",
      "TRAIN -- mse: 0.06782177090644836, nd: 0.012737837620079517, combined: 0.0805596113204956\n",
      "EPOCH 97 started\n",
      "TEST -- mse: 0.06739053875207901, nd: 0.011840837076306343, combined: 0.0792313739657402\n",
      "TRAIN -- mse: 0.06758395582437515, nd: 0.012800311669707298, combined: 0.0803842693567276\n",
      "EPOCH 98 started\n",
      "TEST -- mse: 0.06716637313365936, nd: 0.011906065046787262, combined: 0.07907243818044662\n",
      "TRAIN -- mse: 0.06735263019800186, nd: 0.012862137518823147, combined: 0.08021476864814758\n",
      "EPOCH 99 started\n",
      "TEST -- mse: 0.06694035977125168, nd: 0.011967585422098637, combined: 0.07890794426202774\n",
      "TRAIN -- mse: 0.06712036579847336, nd: 0.012922743335366249, combined: 0.08004310727119446\n",
      "EPOCH 100 started\n",
      "TEST -- mse: 0.06671645492315292, nd: 0.012024237774312496, combined: 0.078740693628788\n",
      "TRAIN -- mse: 0.06689763069152832, nd: 0.012969043105840683, combined: 0.0798666775226593\n",
      "EPOCH 101 started\n",
      "TEST -- mse: 0.06650097668170929, nd: 0.012059559114277363, combined: 0.07856053858995438\n",
      "TRAIN -- mse: 0.06667506694793701, nd: 0.013009629212319851, combined: 0.07968469709157944\n",
      "EPOCH 102 started\n",
      "TEST -- mse: 0.06626058369874954, nd: 0.0121828718110919, combined: 0.07844345271587372\n",
      "TRAIN -- mse: 0.06643636524677277, nd: 0.013111833482980728, combined: 0.0795481950044632\n",
      "EPOCH 103 started\n",
      "TEST -- mse: 0.06602910161018372, nd: 0.012271367013454437, combined: 0.07830046862363815\n",
      "TRAIN -- mse: 0.06620355695486069, nd: 0.013190312311053276, combined: 0.07939387112855911\n",
      "EPOCH 104 started\n",
      "TEST -- mse: 0.06581977009773254, nd: 0.012325655668973923, combined: 0.07814542949199677\n",
      "TRAIN -- mse: 0.06599098443984985, nd: 0.01323560532182455, combined: 0.07922659069299698\n",
      "EPOCH 105 started\n",
      "TEST -- mse: 0.06560033559799194, nd: 0.012403656728565693, combined: 0.07800399512052536\n",
      "TRAIN -- mse: 0.06576385349035263, nd: 0.013309603556990623, combined: 0.0790734589099884\n",
      "EPOCH 106 started\n",
      "TEST -- mse: 0.06539580971002579, nd: 0.012467686086893082, combined: 0.07786349952220917\n",
      "TRAIN -- mse: 0.0655602440237999, nd: 0.013357628136873245, combined: 0.07891787588596344\n",
      "EPOCH 107 started\n",
      "TEST -- mse: 0.0651840791106224, nd: 0.012537245638668537, combined: 0.07772132754325867\n",
      "TRAIN -- mse: 0.06534606963396072, nd: 0.013416055589914322, combined: 0.07876212894916534\n",
      "EPOCH 108 started\n",
      "TEST -- mse: 0.06499049067497253, nd: 0.012548252008855343, combined: 0.07753874361515045\n",
      "TRAIN -- mse: 0.06514818966388702, nd: 0.013432355597615242, combined: 0.07858054339885712\n",
      "EPOCH 109 started\n",
      "TEST -- mse: 0.06480272114276886, nd: 0.012575393542647362, combined: 0.07737811654806137\n",
      "TRAIN -- mse: 0.06495298445224762, nd: 0.013460304588079453, combined: 0.07841329276561737\n",
      "EPOCH 110 started\n",
      "TEST -- mse: 0.06459514796733856, nd: 0.012638825923204422, combined: 0.07723397016525269\n",
      "TRAIN -- mse: 0.06474173069000244, nd: 0.013517227955162525, combined: 0.07825896143913269\n",
      "EPOCH 111 started\n",
      "TEST -- mse: 0.06437942385673523, nd: 0.01268946286290884, combined: 0.07706888765096664\n",
      "TRAIN -- mse: 0.06452110409736633, nd: 0.013577316887676716, combined: 0.07809842377901077\n",
      "EPOCH 112 started\n",
      "TEST -- mse: 0.06420138478279114, nd: 0.01267187763005495, combined: 0.07687326520681381\n",
      "TRAIN -- mse: 0.0643346756696701, nd: 0.013576562516391277, combined: 0.07791123539209366\n",
      "EPOCH 113 started\n",
      "TEST -- mse: 0.06401269137859344, nd: 0.012696543708443642, combined: 0.07670923322439194\n",
      "TRAIN -- mse: 0.06413976103067398, nd: 0.013607853092253208, combined: 0.07774761319160461\n",
      "EPOCH 114 started\n",
      "TEST -- mse: 0.06383714824914932, nd: 0.012703100219368935, combined: 0.07654024660587311\n",
      "TRAIN -- mse: 0.06395633518695831, nd: 0.013619381934404373, combined: 0.07757571339607239\n",
      "EPOCH 115 started\n",
      "TEST -- mse: 0.06364250928163528, nd: 0.01274830661714077, combined: 0.0763908177614212\n",
      "TRAIN -- mse: 0.06375201791524887, nd: 0.013666292652487755, combined: 0.07741831243038177\n",
      "EPOCH 116 started\n",
      "TEST -- mse: 0.06344142556190491, nd: 0.012805840000510216, combined: 0.07624726742506027\n",
      "TRAIN -- mse: 0.06354904919862747, nd: 0.01371600478887558, combined: 0.07726505398750305\n",
      "EPOCH 117 started\n",
      "TEST -- mse: 0.0632355809211731, nd: 0.012900356203317642, combined: 0.07613593339920044\n",
      "TRAIN -- mse: 0.06334471702575684, nd: 0.013783670961856842, combined: 0.07712838798761368\n",
      "EPOCH 118 started\n",
      "TEST -- mse: 0.06304991990327835, nd: 0.012936858460307121, combined: 0.07598678022623062\n",
      "TRAIN -- mse: 0.06314908713102341, nd: 0.013821878470480442, combined: 0.07697096467018127\n",
      "EPOCH 119 started\n",
      "TEST -- mse: 0.06283868104219437, nd: 0.013050587847828865, combined: 0.07588926702737808\n",
      "TRAIN -- mse: 0.06293809413909912, nd: 0.01390768215060234, combined: 0.07684578001499176\n",
      "EPOCH 120 started\n",
      "TEST -- mse: 0.06262946128845215, nd: 0.013139873743057251, combined: 0.0757693350315094\n",
      "TRAIN -- mse: 0.06272770464420319, nd: 0.013982327654957771, combined: 0.07671003043651581\n",
      "EPOCH 121 started\n",
      "TEST -- mse: 0.062458720058202744, nd: 0.013140136376023293, combined: 0.07559885829687119\n",
      "TRAIN -- mse: 0.06255263090133667, nd: 0.013985625468194485, combined: 0.07653825730085373\n",
      "EPOCH 122 started\n",
      "TEST -- mse: 0.06227608025074005, nd: 0.013161556795239449, combined: 0.07543763518333435\n",
      "TRAIN -- mse: 0.06236772984266281, nd: 0.014008156023919582, combined: 0.07637588679790497\n",
      "EPOCH 123 started\n",
      "TEST -- mse: 0.062082841992378235, nd: 0.013221750035881996, combined: 0.07530459016561508\n",
      "TRAIN -- mse: 0.06217225641012192, nd: 0.014060151763260365, combined: 0.07623241096735\n",
      "EPOCH 124 started\n",
      "TEST -- mse: 0.061918508261442184, nd: 0.013196727260947227, combined: 0.07511523365974426\n",
      "TRAIN -- mse: 0.061999887228012085, nd: 0.014051876030862331, combined: 0.07605176419019699\n",
      "EPOCH 125 started\n",
      "TEST -- mse: 0.061741169542074203, nd: 0.013222497887909412, combined: 0.07496366649866104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN -- mse: 0.06181276589632034, nd: 0.014085293747484684, combined: 0.07589805871248245\n",
      "EPOCH 126 started\n",
      "TEST -- mse: 0.06155407056212425, nd: 0.013267943635582924, combined: 0.07482201606035233\n",
      "TRAIN -- mse: 0.06162012740969658, nd: 0.014133413322269917, combined: 0.07575353980064392\n",
      "EPOCH 127 started\n",
      "TEST -- mse: 0.06135977804660797, nd: 0.013335630297660828, combined: 0.0746954083442688\n",
      "TRAIN -- mse: 0.0614241361618042, nd: 0.014192293398082256, combined: 0.07561642676591873\n",
      "EPOCH 128 started\n",
      "TEST -- mse: 0.06118624657392502, nd: 0.013354513794183731, combined: 0.07454076409339905\n",
      "TRAIN -- mse: 0.06124812364578247, nd: 0.01421119924634695, combined: 0.075459323823452\n",
      "EPOCH 129 started\n",
      "TEST -- mse: 0.061005305498838425, nd: 0.013395465910434723, combined: 0.07440076768398285\n",
      "TRAIN -- mse: 0.06106339395046234, nd: 0.014256039634346962, combined: 0.07531943172216415\n",
      "EPOCH 130 started\n",
      "TEST -- mse: 0.06081712618470192, nd: 0.013455179519951344, combined: 0.07427230477333069\n",
      "TRAIN -- mse: 0.06087738275527954, nd: 0.01431166660040617, combined: 0.07518904656171799\n",
      "EPOCH 131 started\n",
      "TEST -- mse: 0.0606318823993206, nd: 0.01352497935295105, combined: 0.07415686547756195\n",
      "TRAIN -- mse: 0.06069154292345047, nd: 0.014371346682310104, combined: 0.07506288588047028\n",
      "EPOCH 132 started\n",
      "TEST -- mse: 0.060427092015743256, nd: 0.013635017909109592, combined: 0.07406210899353027\n",
      "TRAIN -- mse: 0.06048942729830742, nd: 0.014462103135883808, combined: 0.07495152950286865\n",
      "EPOCH 133 started\n",
      "TEST -- mse: 0.06025006249547005, nd: 0.013669424690306187, combined: 0.07391948997974396\n",
      "TRAIN -- mse: 0.06030884385108948, nd: 0.014496301300823689, combined: 0.07480514794588089\n",
      "EPOCH 134 started\n",
      "TEST -- mse: 0.06006817892193794, nd: 0.013730966486036777, combined: 0.07379914820194244\n",
      "TRAIN -- mse: 0.060122050344944, nd: 0.014552967622876167, combined: 0.07467501610517502\n",
      "EPOCH 135 started\n",
      "TEST -- mse: 0.059892259538173676, nd: 0.013787362724542618, combined: 0.07367962598800659\n",
      "TRAIN -- mse: 0.05994190275669098, nd: 0.014596555382013321, combined: 0.074538454413414\n",
      "EPOCH 136 started\n",
      "TEST -- mse: 0.05971642956137657, nd: 0.013832764700055122, combined: 0.07354919612407684\n",
      "TRAIN -- mse: 0.0597609281539917, nd: 0.01463831216096878, combined: 0.07439924031496048\n",
      "EPOCH 137 started\n",
      "TEST -- mse: 0.059543732553720474, nd: 0.013873487710952759, combined: 0.07341721653938293\n",
      "TRAIN -- mse: 0.05958736687898636, nd: 0.014666934497654438, combined: 0.07425430417060852\n",
      "EPOCH 138 started\n",
      "TEST -- mse: 0.05936489254236221, nd: 0.013940076343715191, combined: 0.07330496609210968\n",
      "TRAIN -- mse: 0.05940708890557289, nd: 0.014719489961862564, combined: 0.07412657886743546\n",
      "EPOCH 139 started\n",
      "TEST -- mse: 0.05917828157544136, nd: 0.014028960838913918, combined: 0.07320724427700043\n",
      "TRAIN -- mse: 0.059216562658548355, nd: 0.014793751761317253, combined: 0.07401031255722046\n",
      "EPOCH 140 started\n",
      "TEST -- mse: 0.05902538448572159, nd: 0.014001517556607723, combined: 0.07302690297365189\n",
      "TRAIN -- mse: 0.05905350297689438, nd: 0.014785715378820896, combined: 0.0738392174243927\n",
      "EPOCH 141 started\n",
      "TEST -- mse: 0.05887317657470703, nd: 0.013980448246002197, combined: 0.07285362482070923\n",
      "TRAIN -- mse: 0.05889744684100151, nd: 0.014775180257856846, combined: 0.07367262989282608\n",
      "EPOCH 142 started\n",
      "TEST -- mse: 0.05872231349349022, nd: 0.013960990123450756, combined: 0.07268330454826355\n",
      "TRAIN -- mse: 0.05873967707157135, nd: 0.014769145287573338, combined: 0.07350882142782211\n",
      "EPOCH 143 started\n",
      "TEST -- mse: 0.05855651944875717, nd: 0.014016320928931236, combined: 0.07257284224033356\n",
      "TRAIN -- mse: 0.05857589840888977, nd: 0.014798818156123161, combined: 0.07337471842765808\n",
      "EPOCH 144 started\n",
      "TEST -- mse: 0.0583842471241951, nd: 0.014077617786824703, combined: 0.07246186584234238\n",
      "TRAIN -- mse: 0.058402761816978455, nd: 0.014846157282590866, combined: 0.07324892282485962\n",
      "EPOCH 145 started\n",
      "TEST -- mse: 0.05822937563061714, nd: 0.014079853892326355, combined: 0.0723092257976532\n",
      "TRAIN -- mse: 0.05824103578925133, nd: 0.014856421388685703, combined: 0.07309745997190475\n",
      "EPOCH 146 started\n",
      "TEST -- mse: 0.05806536599993706, nd: 0.01410655491054058, combined: 0.07217191904783249\n",
      "TRAIN -- mse: 0.058071717619895935, nd: 0.014883275143802166, combined: 0.07295498996973038\n",
      "EPOCH 147 started\n",
      "TEST -- mse: 0.057899173349142075, nd: 0.01414454635232687, combined: 0.07204371690750122\n",
      "TRAIN -- mse: 0.05790656432509422, nd: 0.014915497042238712, combined: 0.07282206416130066\n",
      "EPOCH 148 started\n",
      "TEST -- mse: 0.05773487687110901, nd: 0.01418250985443592, combined: 0.07191738486289978\n",
      "TRAIN -- mse: 0.05773814395070076, nd: 0.014955403283238411, combined: 0.07269354909658432\n",
      "EPOCH 149 started\n",
      "TEST -- mse: 0.05755920335650444, nd: 0.014264018274843693, combined: 0.07182322442531586\n",
      "TRAIN -- mse: 0.05756533890962601, nd: 0.01501753181219101, combined: 0.07258287072181702\n",
      "EPOCH 150 started\n",
      "TEST -- mse: 0.05737884342670441, nd: 0.014358568005263805, combined: 0.07173740863800049\n",
      "TRAIN -- mse: 0.057382337749004364, nd: 0.015093990601599216, combined: 0.072476327419281\n",
      "EPOCH 151 started\n",
      "TEST -- mse: 0.057206325232982635, nd: 0.014435604214668274, combined: 0.07164192944765091\n",
      "TRAIN -- mse: 0.05721235275268555, nd: 0.015149705111980438, combined: 0.07236205786466599\n",
      "EPOCH 152 started\n",
      "TEST -- mse: 0.057049140334129333, nd: 0.014447893016040325, combined: 0.07149703055620193\n",
      "TRAIN -- mse: 0.05705060437321663, nd: 0.015170617960393429, combined: 0.07222121953964233\n",
      "EPOCH 153 started\n",
      "TEST -- mse: 0.05689895525574684, nd: 0.014441514387726784, combined: 0.07134047150611877\n",
      "TRAIN -- mse: 0.05689573660492897, nd: 0.01517887506633997, combined: 0.07207461446523666\n",
      "EPOCH 154 started\n",
      "TEST -- mse: 0.05673893913626671, nd: 0.014481346122920513, combined: 0.0712202861905098\n",
      "TRAIN -- mse: 0.05672859773039818, nd: 0.0152194295078516, combined: 0.07194802910089493\n",
      "EPOCH 155 started\n",
      "TEST -- mse: 0.05658527836203575, nd: 0.014504841528832912, combined: 0.07109011709690094\n",
      "TRAIN -- mse: 0.05656788498163223, nd: 0.015242394059896469, combined: 0.0718102753162384\n",
      "EPOCH 156 started\n",
      "TEST -- mse: 0.0564289465546608, nd: 0.014544020406901836, combined: 0.07097296416759491\n",
      "TRAIN -- mse: 0.05641040951013565, nd: 0.015270903706550598, combined: 0.07168131321668625\n",
      "EPOCH 157 started\n",
      "TEST -- mse: 0.05627823248505592, nd: 0.014555884525179863, combined: 0.07083411514759064\n",
      "TRAIN -- mse: 0.056256502866744995, nd: 0.015288317576050758, combined: 0.0715448185801506\n",
      "EPOCH 158 started\n",
      "TEST -- mse: 0.056147459894418716, nd: 0.01449559722095728, combined: 0.07064305990934372\n",
      "TRAIN -- mse: 0.05612269043922424, nd: 0.015251979231834412, combined: 0.07137466967105865\n",
      "EPOCH 159 started\n",
      "TEST -- mse: 0.056008230894804, nd: 0.014474928379058838, combined: 0.07048316299915314\n",
      "TRAIN -- mse: 0.055982496589422226, nd: 0.015237863175570965, combined: 0.07122036069631577\n",
      "EPOCH 160 started\n",
      "TEST -- mse: 0.05589456856250763, nd: 0.014384332112967968, combined: 0.07027889788150787\n",
      "TRAIN -- mse: 0.055861327797174454, nd: 0.01517575979232788, combined: 0.07103708386421204\n",
      "EPOCH 161 started\n",
      "TEST -- mse: 0.05574822798371315, nd: 0.014378516934812069, combined: 0.0701267421245575\n",
      "TRAIN -- mse: 0.05571173503994942, nd: 0.015179737471044064, combined: 0.07089146971702576\n",
      "EPOCH 162 started\n",
      "TEST -- mse: 0.05559368059039116, nd: 0.014437991194427013, combined: 0.07003167271614075\n",
      "TRAIN -- mse: 0.05556323751807213, nd: 0.015206842683255672, combined: 0.07077007740736008\n",
      "EPOCH 163 started\n",
      "TEST -- mse: 0.0554494746029377, nd: 0.014443672262132168, combined: 0.06989314407110214\n",
      "TRAIN -- mse: 0.05541887506842613, nd: 0.01520718913525343, combined: 0.07062606513500214\n",
      "EPOCH 164 started\n",
      "TEST -- mse: 0.055277977138757706, nd: 0.014531330205500126, combined: 0.06980931013822556\n",
      "TRAIN -- mse: 0.055247899144887924, nd: 0.015277650207281113, combined: 0.07052554935216904\n",
      "EPOCH 165 started\n",
      "TEST -- mse: 0.05513244494795799, nd: 0.014538879506289959, combined: 0.06967132538557053\n",
      "TRAIN -- mse: 0.05509766563773155, nd: 0.015290488488972187, combined: 0.07038815319538116\n",
      "EPOCH 166 started\n",
      "TEST -- mse: 0.05498388409614563, nd: 0.014567471109330654, combined: 0.06955135613679886\n",
      "TRAIN -- mse: 0.05494794622063637, nd: 0.015315253287553787, combined: 0.07026319950819016\n",
      "EPOCH 167 started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- mse: 0.054835930466651917, nd: 0.014606069773435593, combined: 0.06944200396537781\n",
      "TRAIN -- mse: 0.054799530655145645, nd: 0.015338506549596786, combined: 0.07013803720474243\n",
      "EPOCH 168 started\n",
      "TEST -- mse: 0.05468929931521416, nd: 0.014626688323915005, combined: 0.06931598484516144\n",
      "TRAIN -- mse: 0.054653216153383255, nd: 0.015352503396570683, combined: 0.07000572234392166\n",
      "EPOCH 169 started\n",
      "TEST -- mse: 0.054551221430301666, nd: 0.014653043821454048, combined: 0.06920426338911057\n",
      "TRAIN -- mse: 0.054507747292518616, nd: 0.015370426699519157, combined: 0.06987817585468292\n",
      "EPOCH 170 started\n",
      "TEST -- mse: 0.054413776844739914, nd: 0.014648287557065487, combined: 0.06906206160783768\n",
      "TRAIN -- mse: 0.05436701700091362, nd: 0.015370508655905724, combined: 0.0697375237941742\n",
      "EPOCH 171 started\n",
      "TEST -- mse: 0.054273821413517, nd: 0.014640258625149727, combined: 0.06891407817602158\n",
      "TRAIN -- mse: 0.0542263500392437, nd: 0.01537407748401165, combined: 0.0696004256606102\n",
      "EPOCH 172 started\n",
      "TEST -- mse: 0.05412573739886284, nd: 0.014663642272353172, combined: 0.06878937780857086\n",
      "TRAIN -- mse: 0.054079774767160416, nd: 0.015396940521895885, combined: 0.06947671622037888\n",
      "EPOCH 173 started\n",
      "TEST -- mse: 0.05396893620491028, nd: 0.014733124524354935, combined: 0.06870205700397491\n",
      "TRAIN -- mse: 0.05392012372612953, nd: 0.015453354455530643, combined: 0.0693734809756279\n",
      "EPOCH 174 started\n",
      "TEST -- mse: 0.05382396653294563, nd: 0.014769984409213066, combined: 0.06859394907951355\n",
      "TRAIN -- mse: 0.05377426743507385, nd: 0.01548217236995697, combined: 0.06925643980503082\n",
      "EPOCH 175 started\n",
      "TEST -- mse: 0.05369110032916069, nd: 0.014765050262212753, combined: 0.06845615059137344\n",
      "TRAIN -- mse: 0.05364077538251877, nd: 0.015475218184292316, combined: 0.06911599636077881\n",
      "EPOCH 176 started\n",
      "TEST -- mse: 0.05355534702539444, nd: 0.014786231331527233, combined: 0.06834157556295395\n",
      "TRAIN -- mse: 0.05350401997566223, nd: 0.015485664829611778, combined: 0.06898968666791916\n",
      "EPOCH 177 started\n",
      "TEST -- mse: 0.053406525403261185, nd: 0.014843140728771687, combined: 0.0682496652007103\n",
      "TRAIN -- mse: 0.05335644632577896, nd: 0.01551610603928566, combined: 0.06887255609035492\n",
      "EPOCH 178 started\n",
      "TEST -- mse: 0.05325052887201309, nd: 0.014937416650354862, combined: 0.06818794459104538\n",
      "TRAIN -- mse: 0.05319890379905701, nd: 0.015579775907099247, combined: 0.06877867877483368\n",
      "EPOCH 179 started\n",
      "TEST -- mse: 0.05310695245862007, nd: 0.014985544607043266, combined: 0.06809249520301819\n",
      "TRAIN -- mse: 0.05305434763431549, nd: 0.015612483024597168, combined: 0.06866683065891266\n",
      "EPOCH 180 started\n",
      "TEST -- mse: 0.052945561707019806, nd: 0.015110062435269356, combined: 0.06805562227964401\n",
      "TRAIN -- mse: 0.05289410799741745, nd: 0.01569577492773533, combined: 0.06858988106250763\n",
      "EPOCH 181 started\n",
      "TEST -- mse: 0.05279486998915672, nd: 0.015184679068624973, combined: 0.06797955185174942\n",
      "TRAIN -- mse: 0.0527426153421402, nd: 0.01574568822979927, combined: 0.06848829984664917\n",
      "EPOCH 182 started\n",
      "TEST -- mse: 0.0526600256562233, nd: 0.015206780284643173, combined: 0.06786680221557617\n",
      "TRAIN -- mse: 0.05260650813579559, nd: 0.0157503392547369, combined: 0.06835684925317764\n",
      "EPOCH 183 started\n",
      "TEST -- mse: 0.05253021791577339, nd: 0.01520409807562828, combined: 0.06773431599140167\n",
      "TRAIN -- mse: 0.052474092692136765, nd: 0.015745267271995544, combined: 0.06821936368942261\n",
      "EPOCH 184 started\n",
      "TEST -- mse: 0.05238037183880806, nd: 0.015283633023500443, combined: 0.0676640048623085\n",
      "TRAIN -- mse: 0.05232908949255943, nd: 0.015790672972798347, combined: 0.06811976432800293\n",
      "EPOCH 185 started\n",
      "TEST -- mse: 0.05224068462848663, nd: 0.015325081534683704, combined: 0.06756576895713806\n",
      "TRAIN -- mse: 0.05219012498855591, nd: 0.015813134610652924, combined: 0.06800325959920883\n",
      "EPOCH 186 started\n",
      "TEST -- mse: 0.052117422223091125, nd: 0.015335106290876865, combined: 0.06745252758264542\n",
      "TRAIN -- mse: 0.0520605593919754, nd: 0.015818433836102486, combined: 0.06787899136543274\n",
      "EPOCH 187 started\n",
      "TEST -- mse: 0.0519946925342083, nd: 0.015308388508856297, combined: 0.06730308383703232\n",
      "TRAIN -- mse: 0.051933638751506805, nd: 0.01580079272389412, combined: 0.06773443520069122\n",
      "EPOCH 188 started\n",
      "TEST -- mse: 0.05185801163315773, nd: 0.015324434265494347, combined: 0.06718244403600693\n",
      "TRAIN -- mse: 0.05179695412516594, nd: 0.01581708714365959, combined: 0.06761404126882553\n",
      "EPOCH 189 started\n",
      "TEST -- mse: 0.05170859396457672, nd: 0.01539583969861269, combined: 0.06710443645715714\n",
      "TRAIN -- mse: 0.05164562165737152, nd: 0.01586879976093769, combined: 0.06751441955566406\n",
      "EPOCH 190 started\n",
      "TEST -- mse: 0.05156731605529785, nd: 0.015447837300598621, combined: 0.0670151561498642\n",
      "TRAIN -- mse: 0.05150020867586136, nd: 0.01590971276164055, combined: 0.06740991771221161\n",
      "EPOCH 191 started\n",
      "TEST -- mse: 0.05142226815223694, nd: 0.01550271175801754, combined: 0.06692498177289963\n",
      "TRAIN -- mse: 0.05134861171245575, nd: 0.015958108007907867, combined: 0.06730671972036362\n",
      "EPOCH 192 started\n",
      "TEST -- mse: 0.051295388489961624, nd: 0.015501623973250389, combined: 0.06679701060056686\n",
      "TRAIN -- mse: 0.05121813714504242, nd: 0.015959927812218666, combined: 0.06717806309461594\n",
      "EPOCH 193 started\n",
      "TEST -- mse: 0.051178593188524246, nd: 0.015468026511371136, combined: 0.06664662063121796\n",
      "TRAIN -- mse: 0.051096539944410324, nd: 0.01593584381043911, combined: 0.06703238189220428\n",
      "EPOCH 194 started\n",
      "TEST -- mse: 0.05103794485330582, nd: 0.015531430952250957, combined: 0.06656937301158905\n",
      "TRAIN -- mse: 0.05095216631889343, nd: 0.015975743532180786, combined: 0.06692790985107422\n",
      "EPOCH 195 started\n",
      "TEST -- mse: 0.05092470347881317, nd: 0.015507166273891926, combined: 0.06643187254667282\n",
      "TRAIN -- mse: 0.05083682760596275, nd: 0.015949448570609093, combined: 0.0667862743139267\n",
      "EPOCH 196 started\n",
      "TEST -- mse: 0.05079955980181694, nd: 0.015520418994128704, combined: 0.06631997972726822\n",
      "TRAIN -- mse: 0.050710298120975494, nd: 0.01595068909227848, combined: 0.06666098535060883\n",
      "EPOCH 197 started\n",
      "TEST -- mse: 0.05065272003412247, nd: 0.015587838366627693, combined: 0.06624055653810501\n",
      "TRAIN -- mse: 0.05056231841444969, nd: 0.016004472970962524, combined: 0.06656679511070251\n",
      "EPOCH 198 started\n",
      "TEST -- mse: 0.050519924610853195, nd: 0.015608829446136951, combined: 0.06612875312566757\n",
      "TRAIN -- mse: 0.050427988171577454, nd: 0.016021816059947014, combined: 0.06644980609416962\n",
      "EPOCH 199 started\n",
      "TEST -- mse: 0.05036759376525879, nd: 0.01569787599146366, combined: 0.0660654678940773\n",
      "TRAIN -- mse: 0.05027346685528755, nd: 0.01609068550169468, combined: 0.06636415421962738\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = NetRegression(d, 1)\n",
    "num_epochs = 200\n",
    "lr = 1e-5\n",
    "fairness_weight = 1\n",
    "num_constrained_intervals = 2\n",
    "intervals = generate_constrained_intervals(num_constrained_intervals)\n",
    "fairness_metric_train = generate_fairness_metric(intervals, intervals, True)\n",
    "fairness_metric_test = generate_fairness_metric(intervals, intervals, False)\n",
    "\n",
    "model = regularized_learning(x_train, y_train, a_train, model=model, fairness_metric_train=fairness_metric_train, fairness_metric_test=fairness_metric_test, lr=lr, \\\n",
    "                             num_epochs=num_epochs, fairness_weight=fairness_weight)\n",
    "mse, discrimination = evaluate(model, x_test, y_test, a_test, fairness_metric=fairness_metric_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:0.06243539974093437 Beta loss:0.015345253981649876\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE:{} Beta loss:{}\".format(mse, discrimination))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
